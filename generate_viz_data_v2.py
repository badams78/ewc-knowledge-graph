#!/usr/bin/env python3
"""
Generate visualization data for V2 taxonomy (1536d embeddings).
"""

import os
import json
import numpy as np
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import cosine_similarity
from pathlib import Path

OUTPUT_DIR = Path(r"C:\Users\Brandon Adams\EWC_Database_v2")
DATA_DIR = OUTPUT_DIR / "taxonomy_output_v2" / "data"

def load_v2_data():
    """Load data generated by taxonomy_development_v2.py"""
    print("Loading v2 taxonomy data...")
    
    # Load taxonomy structure
    with open(OUTPUT_DIR / "derived_taxonomy_v2.json", "r", encoding='utf-8') as f:
        taxonomy = json.load(f)
        
    # Load embeddings (for centroid calculation)
    # Check for NPZ
    if (OUTPUT_DIR / "fulltext_embeddings.npz").exists():
        data = np.load(OUTPUT_DIR / "fulltext_embeddings.npz", allow_pickle=True)
        post_ids = data['post_ids'].tolist()
        embeddings = data['embeddings']
        embedding_map = {pid: emb for pid, emb in zip(post_ids, embeddings)}
    else:
        raise FileNotFoundError("fulltext_embeddings.npz missing")
        
    # Load assignments
    with open(OUTPUT_DIR / "cluster_assignments_v2.json", "r", encoding='utf-8') as f:
        assignments = json.load(f)
    
    return taxonomy, embedding_map, assignments

def calculate_centroids(taxonomy, embedding_map, assignments):
    """Calculate centroids for all Tier 2 subclusters"""
    print("Calculating subcluster centroids...")
    
    # Group post_ids by subcluster
    # Key: "T1-X_T2-Y" -> [post_id1, post_id2...]
    groups = {}
    
    for item in assignments:
        c_id = item['cluster']
        s_id = item['subcluster']
        key = f"T1-{c_id}_T2-{s_id}"
        
        if key not in groups:
            groups[key] = []
        groups[key].append(item['post_id'])
    
    subclusters = []
    
    # Iterate through taxonomy to ensure order/completeness
    for c_key, c_data in taxonomy['clusters'].items():
        # c_key is "Cluster_0" -> T1_ID = 0
        t1_id = int(c_key.split('_')[1])
        
        t1_topics = c_data.get('top_existing_topics', [])
        if t1_topics:
            primary_name = t1_topics[0][0]
        else:
            t1_cats = c_data.get('top_existing_categories', [])
            if t1_cats:
                primary_name = t1_cats[0][0]
            else:
                primary_name = f"Cluster {t1_id}"
        
        for t2_key, t2_data in c_data.get('tier2_clusters', {}).items():
            # t2_key is "T1-0_T2-0"
            
            # Get articles
            pids = groups.get(t2_key, [])
            if not pids:
                print(f"Warning: No articles found for {t2_key}")
                continue
                
            # Compute centroid
            embs = [embedding_map[pid] for pid in pids if pid in embedding_map]
            if not embs:
                continue
            
            centroid = np.mean(embs, axis=0)
            
            # Name heuristic: Use top existing topic + category
            t2_topics = t2_data.get('top_existing_topics', [])
            t2_cats = t2_data.get('top_existing_categories', [])
            
            name_parts = []
            if t2_topics: name_parts.append(t2_topics[0][0])
            if t2_cats: name_parts.append(t2_cats[0][0])
            name = " & ".join(name_parts) if name_parts else f"Subcluster {t2_key}"
            
            subclusters.append({
                'id': t2_key,
                'name': name,
                'primary_id': f"P{t1_id}",
                'primary_name': primary_name,
                'article_count': t2_data['article_count'],
                'centroid': centroid
            })
            
    print(f"Calculated centroids for {len(subclusters)} subclusters")
    return subclusters

def main():
    os.makedirs(DATA_DIR, exist_ok=True)
    
    # 1. Load Data
    taxonomy, embedding_map, assignments = load_v2_data()
    
    # 2. Calculate Centroids
    all_subclusters = calculate_centroids(taxonomy, embedding_map, assignments)
    centroids = np.array([s['centroid'] for s in all_subclusters])
    
    # 3. Similarity Matrix
    print("Computing similarity matrix...")
    similarity_matrix = cosine_similarity(centroids)
    
    sim_data = {
        'subclusters': [s['id'] for s in all_subclusters],
        'matrix': similarity_matrix.tolist()
    }
    with open(DATA_DIR / "similarity_matrix.json", "w") as f:
        json.dump(sim_data, f)
        
    # 4. t-SNE Coordinates
    print("Computing t-SNE...")
    n_samples = len(centroids)
    perplexity = min(30, n_samples - 1)
    
    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, init='pca', learning_rate='auto')
    coords_2d = tsne.fit_transform(centroids)
    
    # Normalize 0-100
    x_min, x_max = coords_2d[:, 0].min(), coords_2d[:, 0].max()
    y_min, y_max = coords_2d[:, 1].min(), coords_2d[:, 1].max()
    
    coords_2d[:, 0] = (coords_2d[:, 0] - x_min) / (x_max - x_min) * 100
    coords_2d[:, 1] = (coords_2d[:, 1] - y_min) / (y_max - y_min) * 100
    
    coords_data = []
    for i, sub in enumerate(all_subclusters):
        coords_data.append({
            'id': sub['id'],
            'name': sub['name'],
            'primary_id': sub['primary_id'],
            'primary_name': sub['primary_name'],
            'article_count': sub['article_count'],
            'x': float(coords_2d[i, 0]),
            'y': float(coords_2d[i, 1])
        })
        
    with open(DATA_DIR / "coords_2d.json", "w") as f:
        json.dump(coords_data, f, indent=2)
        
    # 5. Colors
    print("Generating colors...")
    # Distinct colors
    colors_list = [
        '#E63946', '#457B9D', '#2A9D8F', '#E9C46A', '#F4A261', 
        '#264653', '#8338EC', '#FF006E', '#3A86FF', '#06D6A0', 
        '#118AB2', '#EF476F', '#FFD166', '#073B4C', '#9B5DE5'
    ]
    
    primary_names = sorted(list(set(s['primary_name'] for s in all_subclusters)))
    primary_colors = {}
    
    # Re-do specific color mapping based on Cluster ID
    primary_config = []
    by_name_colors = {}
    
    for c_key, c_data in taxonomy['clusters'].items():
        # c_key: Cluster_0
        idx = int(c_key.split('_')[1])
        
        t1_topics = c_data.get('top_existing_topics', [])
        if t1_topics:
            name = t1_topics[0][0]
        else:
            t1_cats = c_data.get('top_existing_categories', [])
            if t1_cats:
                name = t1_cats[0][0]
            else:
                name = f"Cluster {idx}"
        
        color = colors_list[idx % len(colors_list)]
        
        primary_config.append({
            'id': f"P{idx}",
            'name': name,
            'color': color,
            'subclusters': [
                 {'id': s['id'], 'name': s['name'], 'article_count': s['article_count']}
                 for s in all_subclusters if s['primary_id'] == f"P{idx}"
            ]
        })
        by_name_colors[name] = color
        by_name_colors[f"P{idx}"] = color
        
    color_data = {
        'primaries': primary_config,
        'by_name': by_name_colors
    }
    
    with open(DATA_DIR / "colors.json", "w") as f:
        json.dump(color_data, f, indent=2)

    # 6. Spider Data
    print("Computing spider data...")
    spider_data = []
    
    # Map primary name to index in similarity matrix?
    # No, spider chart needs avg similarity to each PRIMARY cluster.
    
    # Pre-calculate primary centroids
    primary_centroids = {}
    for c_key, c_data in taxonomy['clusters'].items():
        idx = int(c_key.split('_')[1])
        
        t1_topics = c_data.get('top_existing_topics', [])
        if t1_topics:
            name = t1_topics[0][0]
        else:
            t1_cats = c_data.get('top_existing_categories', [])
            if t1_cats:
                name = t1_cats[0][0]
            else:
                name = f"Cluster {idx}"
        
        # Get all articles in this primary cluster
        pids = []
        for item in assignments:
            if item['cluster'] == idx:
                pids.append(item['post_id'])
                
        embs = [embedding_map[pid] for pid in pids if pid in embedding_map]
        if embs:
            primary_centroids[name] = np.mean(embs, axis=0)
            
    for sub in all_subclusters:
        sub_vec = sub['centroid'].reshape(1, -1)
        sims = {}
        for pname, pvec in primary_centroids.items():
            sim = cosine_similarity(sub_vec, pvec.reshape(1, -1))[0][0]
            sims[pname] = float(sim)
            
        spider_data.append({
            'id': sub['id'],
            'name': sub['name'],
            'primary_name': sub['primary_name'],
            'similarities': sims
        })
        
    with open(DATA_DIR / "spider_data.json", "w") as f:
        json.dump(spider_data, f, indent=2)
        
    # 7. Metadata
    metadata_out = {
        'total_subclusters': len(all_subclusters),
        'total_primaries': len(taxonomy['clusters']),
        'primaries': primary_config,
        'subcluster_ids': [s['id'] for s in all_subclusters]
    }
    with open(DATA_DIR / "metadata.json", "w") as f:
        json.dump(metadata_out, f, indent=2)
        
    # 8. Per-subcluster files
    sub_dir = DATA_DIR / "subclusters"
    os.makedirs(sub_dir, exist_ok=True)
    
    for i, sub in enumerate(all_subclusters):
        # Find related
        sim_row = similarity_matrix[i]
        related = []
        for j, other in enumerate(all_subclusters):
            if i == j: continue
            related.append({
                'id': other['id'],
                'name': other['name'],
                'primary_name': other['primary_name'],
                'similarity': float(sim_row[j])
            })
        related.sort(key=lambda x: x['similarity'], reverse=True)
        
        out = {
            'id': sub['id'],
            'name': sub['name'],
            'primary_id': sub['primary_id'],
            'primary_name': sub['primary_name'],
            'article_count': sub['article_count'],
            'coords': {'x': coords_data[i]['x'], 'y': coords_data[i]['y']},
            'spider': spider_data[i]['similarities'],
            'related': related[:20] 
        }
        with open(sub_dir / f"{sub['id']}.json", "w") as f:
            json.dump(out, f, indent=2)

    print("Generation complete!")

if __name__ == "__main__":
    main()
